# COPY THIS FILE TO `.ENV` AND FILL IN YOUR API KEYS AND SETTINGS.
# LiteLLM enables 100+ providers.
# For supported providers and models head to https://models.litellm.ai/ or https://docs.litellm.ai/docs/providers

# LLM DETECTOR NODE
# LiteLLM provider identifier
LLM_PROVIDER=openai
# Model name for the provider
LLM_MODEL=o3-mini
# API key required by the provider
LLM_API_KEY=your-provider-api-key
# Optional. Provider base URL. Default for Ollama: http://localhost:11434
LLM_BASE_URL=https://api.openai.com/v1
# Optional: version string required by some providers
LLM_API_VERSION=2024-05-01
# Optional: extra LiteLLM params (see https://docs.litellm.ai/docs/completion/input)
LLM_EXTRA_PARAMS={"timeout":120, "temperature":0.2, "seed":12341}

# LLM OCR DETECTOR (optional, fallbacks to base LLM_ values if not stated)
LLM_OCR_PROVIDER=groq
LLM_OCR_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
LLM_OCR_API_KEY=your-provider-api-key
LLM_OCR_BASE_URL=https://api.groq.com/openai/v1

# Backend HTTP port
PORT=8000
# Minimum risk level to block (low, medium, high)
MIN_BLOCK_RISK=medium

# OCR Configuration (requires Tesseract binary installed on system) (optional)
OCR_LANG=eng
# Minimum confidence threshold 0-100 (optional)
OCR_CONFIDENCE_THRESHOLD=0
# Custom tesseract binary path (optional)
TESSERACT_CMD=/usr/local/bin/tesseract
