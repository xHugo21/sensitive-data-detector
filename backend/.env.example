# ============================================================================
# Sensitive Data Detector - Backend Configuration
# ============================================================================

# LLM Configuration (Primary)
# ---------------------------
# LiteLLM provider identifier (e.g., openai, anthropic, ollama)
LLM_PROVIDER=openai

# Model name for the provider
LLM_MODEL=o3-mini

# API key required by the provider
LLM_API_KEY=your-provider-api-key

# Optional. Provider base URL. Default for Ollama: http://localhost:11434
LLM_BASE_URL=https://api.openai.com/v1

# Optional: version string required by some providers
LLM_API_VERSION=2024-05-01

# Optional: extra LiteLLM params (see https://docs.litellm.ai/docs/completion/input)
LLM_EXTRA_PARAMS={"temperature":0, "top_p":1.0, "frequency_penalty":0, "presence_penalty":0, "repeat_penalty":1.0, "top_k":1, "drop_params":true, "response_format":{"type":"json_object"}}

# Force LLM detector to always run even if decision is already 'block'
FORCE_LLM_DETECTOR=false

# LLM Configuration (OCR Fallback)
# --------------------------------
# Optional, fallbacks to base LLM_ values if not stated
LLM_OCR_PROVIDER=groq
LLM_OCR_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
LLM_OCR_API_KEY=your-provider-api-key
LLM_OCR_BASE_URL=https://api.groq.com/openai/v1

# Backend Service Configuration
# -----------------------------
# Backend HTTP port
PORT=8000

# Minimum risk level to block (low, medium, high)
MIN_BLOCK_RISK=medium

# OCR Engine Configuration (Tesseract)
# ------------------------------------
# Requires Tesseract binary installed on system (optional)
OCR_LANG=eng

# Minimum confidence threshold 0-100 (optional)
OCR_CONFIDENCE_THRESHOLD=0

# Custom tesseract binary path (optional)
TESSERACT_CMD=/usr/local/bin/tesseract

# NER Configuration
# -----------------
# Named Entity Recognition settings (optional)
NER_ENABLED=false
NER_MODEL=urchade/gliner_multi-v2.1
NER_MIN_SCORE=0.7
