# COPY THIS FILE TO `.ENV` AND FILL IN YOUR API KEYS AND SETTINGS.
# LiteLLM enables 100+ providers.
# For supported providers and models head to https://models.litellm.ai/ or https://docs.litellm.ai/docs/providers

LLM_PROVIDER=openai                    # LiteLLM provider identifier
LLM_MODEL=o3-mini                      # Model name for the provider
LLM_API_KEY=your-provider-api-key      # API key required by the provider
LLM_BASE_URL=https://api.openai.com/v1 # Provider base URL. Default for Ollama: http://localhost:11434
# LLM_API_VERSION=2024-05-01                                      # Optional: version string required by some providers
# LLM_EXTRA_PARAMS={"timeout":120, "temperature":0.2, "seed":12341} # Optional: extra LiteLLM params (see https://docs.litellm.ai/docs/completion/input)
# LLM_SUPPORTS_JSON_MODE=true                                     # Optional: true when provider honors OpenAI-style JSON mode

PORT=8000 # Backend HTTP port
